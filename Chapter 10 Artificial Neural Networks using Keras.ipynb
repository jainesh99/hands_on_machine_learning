{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.load_session('ch10ann.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a single Threshold Linear Unit (TLU) which is known as a perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted [0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data[:, (2,3)] #petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "print('Predicted', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.1.0\n",
      "Keras version:  2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print('Tensorflow version: ',tf.__version__)\n",
    "print('Keras version: ',keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image classification using Fashion Mnist, this is different from Mnist as Mnist was digits [0,1,2,3,4,5...9] whereas Fashion Mnist is clothing: Tshirt, trouser, pullover etc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape\n",
    "#You have 60000 28 by 28 elements in teh array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the pixel intensities, since all are from 0 to 255 -> 0 to 1, also creating a validation set\n",
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 7, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \n",
    "               \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets build our MLP\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28])) #Convert input to a 1D array\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\")) #300 Neurons with Relu activation function\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "#All of this can also be parsed as paramters inside Sequential\n",
    "#model = keras.models.Sequential([\n",
    "#    keras.layers.Flatten(input_shape=[28,28]),\n",
    "#    keras.layers.Dense(300, activation=\"relu\"),\n",
    "#    keras.layers.Dense(100, activation=\"relu\"),\n",
    "#    keras.layers.Dense(10, activation=\"softmax\")\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x15272f860>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x15270b4a8>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x15270b748>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x15261d630>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense') is hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00064065, -0.05406202,  0.06755719, ...,  0.04977856,\n",
       "         0.0735604 ,  0.00520879],\n",
       "       [ 0.03916007,  0.05973338, -0.02302288, ...,  0.06761892,\n",
       "         0.06866913,  0.00079568],\n",
       "       [ 0.06503299, -0.05525991,  0.05248441, ..., -0.01200636,\n",
       "        -0.01764258, -0.03057273],\n",
       "       ...,\n",
       "       [-0.0018555 , -0.02894144, -0.06312636, ..., -0.07225871,\n",
       "         0.05890697,  0.00673521],\n",
       "       [ 0.03758623,  0.02471546, -0.02648306, ..., -0.07403474,\n",
       "        -0.07191742,  0.0514216 ],\n",
       "       [-0.00858812,  0.02415555,  0.02097904, ..., -0.03425112,\n",
       "        -0.05430547, -0.02035681]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "#Can also use other ones which is the same and do not have to write the actual word for instance\n",
    "# sgd = keras.optimizers.SGD(), default learning rate = 0.01\n",
    "# Using sparse becuase we have 1 to 9 and it has not been one hot encoded\n",
    "# Can be converted have to use keras.utils.to_categorical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==============================] - 4s 75us/sample - loss: 0.1372 - accuracy: 0.9511 - val_loss: 0.3114 - val_accuracy: 0.8964\n",
      "Epoch 2/30\n",
      "55000/55000 [==============================] - 4s 71us/sample - loss: 0.1340 - accuracy: 0.9528 - val_loss: 0.3108 - val_accuracy: 0.9004\n",
      "Epoch 3/30\n",
      "55000/55000 [==============================] - 4s 70us/sample - loss: 0.1334 - accuracy: 0.9534 - val_loss: 0.3062 - val_accuracy: 0.8952\n",
      "Epoch 4/30\n",
      "55000/55000 [==============================] - 4s 70us/sample - loss: 0.1304 - accuracy: 0.9546 - val_loss: 0.3128 - val_accuracy: 0.8958\n",
      "Epoch 5/30\n",
      "55000/55000 [==============================] - 4s 72us/sample - loss: 0.1281 - accuracy: 0.9545 - val_loss: 0.3118 - val_accuracy: 0.8990\n",
      "Epoch 6/30\n",
      "55000/55000 [==============================] - 4s 72us/sample - loss: 0.1260 - accuracy: 0.9558 - val_loss: 0.3192 - val_accuracy: 0.8974\n",
      "Epoch 7/30\n",
      "55000/55000 [==============================] - 4s 76us/sample - loss: 0.1258 - accuracy: 0.9558 - val_loss: 0.3084 - val_accuracy: 0.8992\n",
      "Epoch 8/30\n",
      "55000/55000 [==============================] - 4s 75us/sample - loss: 0.1208 - accuracy: 0.9577 - val_loss: 0.3110 - val_accuracy: 0.9016\n",
      "Epoch 9/30\n",
      "55000/55000 [==============================] - 4s 69us/sample - loss: 0.1196 - accuracy: 0.9584 - val_loss: 0.3075 - val_accuracy: 0.8976\n",
      "Epoch 10/30\n",
      "55000/55000 [==============================] - 4s 71us/sample - loss: 0.1171 - accuracy: 0.9597 - val_loss: 0.3093 - val_accuracy: 0.8986\n",
      "Epoch 11/30\n",
      "55000/55000 [==============================] - 4s 69us/sample - loss: 0.1171 - accuracy: 0.9592 - val_loss: 0.3114 - val_accuracy: 0.8956\n",
      "Epoch 12/30\n",
      "55000/55000 [==============================] - 4s 72us/sample - loss: 0.1141 - accuracy: 0.9600 - val_loss: 0.3171 - val_accuracy: 0.8962\n",
      "Epoch 13/30\n",
      "55000/55000 [==============================] - 4s 75us/sample - loss: 0.1111 - accuracy: 0.9613 - val_loss: 0.3328 - val_accuracy: 0.8930\n",
      "Epoch 14/30\n",
      "55000/55000 [==============================] - 4s 72us/sample - loss: 0.1107 - accuracy: 0.9613 - val_loss: 0.3203 - val_accuracy: 0.8962\n",
      "Epoch 15/30\n",
      "55000/55000 [==============================] - 4s 69us/sample - loss: 0.1082 - accuracy: 0.9622 - val_loss: 0.3226 - val_accuracy: 0.8940\n",
      "Epoch 16/30\n",
      "55000/55000 [==============================] - 4s 70us/sample - loss: 0.1068 - accuracy: 0.9620 - val_loss: 0.3232 - val_accuracy: 0.8946\n",
      "Epoch 17/30\n",
      "55000/55000 [==============================] - 4s 71us/sample - loss: 0.1045 - accuracy: 0.9642 - val_loss: 0.3258 - val_accuracy: 0.8960\n",
      "Epoch 18/30\n",
      "55000/55000 [==============================] - 4s 71us/sample - loss: 0.1029 - accuracy: 0.9639 - val_loss: 0.3150 - val_accuracy: 0.9010\n",
      "Epoch 19/30\n",
      "55000/55000 [==============================] - 4s 70us/sample - loss: 0.1022 - accuracy: 0.9641 - val_loss: 0.3502 - val_accuracy: 0.8948\n",
      "Epoch 20/30\n",
      "55000/55000 [==============================] - 4s 71us/sample - loss: 0.1002 - accuracy: 0.9648 - val_loss: 0.3267 - val_accuracy: 0.8980\n",
      "Epoch 21/30\n",
      "55000/55000 [==============================] - 4s 71us/sample - loss: 0.0982 - accuracy: 0.9665 - val_loss: 0.3268 - val_accuracy: 0.8990\n",
      "Epoch 22/30\n",
      "55000/55000 [==============================] - 4s 72us/sample - loss: 0.0963 - accuracy: 0.9668 - val_loss: 0.3409 - val_accuracy: 0.8978\n",
      "Epoch 23/30\n",
      "55000/55000 [==============================] - 4s 72us/sample - loss: 0.0949 - accuracy: 0.9681 - val_loss: 0.3328 - val_accuracy: 0.8952\n",
      "Epoch 24/30\n",
      "55000/55000 [==============================] - 4s 70us/sample - loss: 0.0931 - accuracy: 0.9676 - val_loss: 0.3571 - val_accuracy: 0.8900\n",
      "Epoch 25/30\n",
      "55000/55000 [==============================] - 4s 70us/sample - loss: 0.0918 - accuracy: 0.9683 - val_loss: 0.3342 - val_accuracy: 0.9006\n",
      "Epoch 26/30\n",
      "55000/55000 [==============================] - 4s 70us/sample - loss: 0.0906 - accuracy: 0.9690 - val_loss: 0.3509 - val_accuracy: 0.8962\n",
      "Epoch 27/30\n",
      "55000/55000 [==============================] - 4s 70us/sample - loss: 0.0889 - accuracy: 0.9694 - val_loss: 0.3510 - val_accuracy: 0.8958\n",
      "Epoch 28/30\n",
      "55000/55000 [==============================] - 4s 70us/sample - loss: 0.0873 - accuracy: 0.9698 - val_loss: 0.3314 - val_accuracy: 0.8972\n",
      "Epoch 29/30\n",
      "55000/55000 [==============================] - 4s 71us/sample - loss: 0.0854 - accuracy: 0.9713 - val_loss: 0.3456 - val_accuracy: 0.8966\n",
      "Epoch 30/30\n",
      "55000/55000 [==============================] - 4s 70us/sample - loss: 0.0843 - accuracy: 0.9713 - val_loss: 0.3449 - val_accuracy: 0.8964\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))\n",
    "# Can also use validation split = 0.1 which tells keras to use the last 10% of the data (before shuffling) for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1) #Set the vertical range from 0 to 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.3764 - accuracy: 0.8873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37639594252556563, 0.8873]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.  , 0.99, 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes:  [9 2 1]\n",
      "['Ankle boot' 'Pullover' 'Trouser']\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "print(\"Predicted classes: \", y_pred)\n",
    "print(np.array(class_names)[y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a MLP for a regression dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # Have to do this first for the scaler to understand the data\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=keras.activations.relu, input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1) #output layer\n",
    "])\n",
    "\n",
    "model.compile(loss=keras.metrics.mean_squared_error, optimizer=keras.optimizers.SGD())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.8158 - val_loss: 0.5936\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 1.0327 - val_loss: 0.5235\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4405 - val_loss: 0.4457\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3980 - val_loss: 0.4302\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.3977 - val_loss: 0.4285\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.4260 - val_loss: 0.4650\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4070 - val_loss: 0.4298\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.3817 - val_loss: 0.4221\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.3728 - val_loss: 0.4165\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.3711 - val_loss: 0.4127\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.3677 - val_loss: 0.4078\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.3742 - val_loss: 0.4103\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.3643 - val_loss: 0.4104\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3604 - val_loss: 0.4093\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3608 - val_loss: 0.3978\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3630 - val_loss: 0.3993\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.3569 - val_loss: 0.3983\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3529 - val_loss: 0.3964\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.3539 - val_loss: 0.3964\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.3504 - val_loss: 0.4048\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 24us/sample - loss: 0.3544\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted is:  [[1.1735533 ]\n",
      " [0.81437635]\n",
      " [1.8515328 ]]\n",
      "The actual values are:  [1.375 0.699 1.688]\n"
     ]
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)\n",
    "print(\"The predicted is: \", y_pred)\n",
    "print(\"The actual values are: \", y_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another form of a neural network called wide and deep, where in the wide it goes from one layer straight to the output and deep means it goes through all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=keras.activations.relu)(input_) # Functional API, telling keras how to connect the layers together\n",
    "hidden2 = keras.layers.Dense(30, activation=keras.activations.relu)(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_,hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However what happens if we want to split the features through the wide path and then through the deep path. Use a subset of features for the one path and a second subset for the next path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=keras.activations.relu)(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=keras.activations.relu)(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4432588 , -0.18146758,  0.14658918, -0.00668425, -1.30602918,\n",
       "         1.19403516],\n",
       "       [ 0.02243357, -0.07394808,  3.97718582,  0.06524645,  1.03325755,\n",
       "        -0.86489745],\n",
       "       [ 0.49638274, -0.15580488, -0.52270705, -0.08633485, -0.93099323,\n",
       "         0.82833666],\n",
       "       ...,\n",
       "       [-0.45379735, -0.16914238,  0.4076059 ,  0.02796317, -1.36697252,\n",
       "         1.2341117 ],\n",
       "       [-0.10269501, -0.02855418, -0.88513229, -0.12211768,  2.55215315,\n",
       "        -2.29763401],\n",
       "       [-0.44231162,  0.01946478, -0.18673622,  0.00760211, -0.7575391 ,\n",
       "         0.64298263]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.79641367, -0.37370348,  0.4432588 , -0.18146758,  0.14658918],\n",
       "       [-0.10858498, -1.48584374,  0.02243357, -0.07394808,  3.97718582],\n",
       "       [ 1.73179495, -0.05594913,  0.49638274, -0.15580488, -0.52270705],\n",
       "       ...,\n",
       "       [-1.2989785 , -0.53258066, -0.45379735, -0.16914238,  0.4076059 ],\n",
       "       [-0.4427939 ,  0.26180523, -0.10269501, -0.02855418, -0.88513229],\n",
       "       [-1.33541641,  1.13562972, -0.44231162,  0.01946478, -0.18673622]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, however you would need to pass in multiple values for X_train and X_test etc => Call it X_train_A, X_train_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.metrics.mean_squared_error, optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 2.0047 - val_loss: 0.9863\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.8285 - val_loss: 0.7748\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.7054 - val_loss: 0.7131\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.6577 - val_loss: 0.6797\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.6293 - val_loss: 0.6598\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.6057 - val_loss: 0.6353\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5877 - val_loss: 0.6192\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5703 - val_loss: 0.6045\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5576 - val_loss: 0.5913\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5437 - val_loss: 0.5794\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5324 - val_loss: 0.5685\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5234 - val_loss: 0.5592\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.5155 - val_loss: 0.5528\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.5065 - val_loss: 0.5481\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.5017 - val_loss: 0.5406\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4948 - val_loss: 0.5339\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4895 - val_loss: 0.5324\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4846 - val_loss: 0.5261\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4792 - val_loss: 0.5203\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4757 - val_loss: 0.5175\n"
     ]
    }
   ],
   "source": [
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20, validation_data=((X_valid_A, X_valid_B),y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 26us/sample - loss: 0.4718\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If need be you could have multiple outputs, can be one for classification or something to see that your model is actually learning something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[keras.metrics.mse, keras.metrics.mse], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 94us/sample - loss: 0.6987 - main_output_loss: 0.5940 - aux_output_loss: 1.6402 - val_loss: 0.5734 - val_main_output_loss: 0.5159 - val_aux_output_loss: 1.0921\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.5640 - main_output_loss: 0.5149 - aux_output_loss: 1.0057 - val_loss: 0.5327 - val_main_output_loss: 0.4872 - val_aux_output_loss: 0.9426\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4859 - main_output_loss: 0.4429 - aux_output_loss: 0.8722 - val_loss: 0.5198 - val_main_output_loss: 0.4844 - val_aux_output_loss: 0.8397\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4646 - main_output_loss: 0.4306 - aux_output_loss: 0.7698 - val_loss: 0.4977 - val_main_output_loss: 0.4692 - val_aux_output_loss: 0.7550\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4455 - main_output_loss: 0.4163 - aux_output_loss: 0.7099 - val_loss: 0.4810 - val_main_output_loss: 0.4558 - val_aux_output_loss: 0.7089\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4397 - main_output_loss: 0.4148 - aux_output_loss: 0.6659 - val_loss: 0.4695 - val_main_output_loss: 0.4474 - val_aux_output_loss: 0.6692\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.4280 - main_output_loss: 0.4049 - aux_output_loss: 0.6347 - val_loss: 0.4608 - val_main_output_loss: 0.4405 - val_aux_output_loss: 0.6444\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.4142 - main_output_loss: 0.3920 - aux_output_loss: 0.6128 - val_loss: 0.4677 - val_main_output_loss: 0.4481 - val_aux_output_loss: 0.6454\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4078 - main_output_loss: 0.3865 - aux_output_loss: 0.5997 - val_loss: 0.4472 - val_main_output_loss: 0.4283 - val_aux_output_loss: 0.6181\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4032 - main_output_loss: 0.3827 - aux_output_loss: 0.5881 - val_loss: 0.4394 - val_main_output_loss: 0.4213 - val_aux_output_loss: 0.6029\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3948 - main_output_loss: 0.3749 - aux_output_loss: 0.5744 - val_loss: 0.4354 - val_main_output_loss: 0.4177 - val_aux_output_loss: 0.5950\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3903 - main_output_loss: 0.3710 - aux_output_loss: 0.5662 - val_loss: 0.4335 - val_main_output_loss: 0.4170 - val_aux_output_loss: 0.5832\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3840 - main_output_loss: 0.3651 - aux_output_loss: 0.5533 - val_loss: 0.4322 - val_main_output_loss: 0.4161 - val_aux_output_loss: 0.5780\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4005 - main_output_loss: 0.3840 - aux_output_loss: 0.5495 - val_loss: 0.4252 - val_main_output_loss: 0.4095 - val_aux_output_loss: 0.5677\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4250 - main_output_loss: 0.4100 - aux_output_loss: 0.5606 - val_loss: 0.4315 - val_main_output_loss: 0.4143 - val_aux_output_loss: 0.5875\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3918 - main_output_loss: 0.3743 - aux_output_loss: 0.5484 - val_loss: 0.4171 - val_main_output_loss: 0.4005 - val_aux_output_loss: 0.5671\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3843 - main_output_loss: 0.3685 - aux_output_loss: 0.5258 - val_loss: 0.4132 - val_main_output_loss: 0.3986 - val_aux_output_loss: 0.5454\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3676 - main_output_loss: 0.3516 - aux_output_loss: 0.5119 - val_loss: 0.4102 - val_main_output_loss: 0.3962 - val_aux_output_loss: 0.5371\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3626 - main_output_loss: 0.3471 - aux_output_loss: 0.5026 - val_loss: 0.4092 - val_main_output_loss: 0.3954 - val_aux_output_loss: 0.5342\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3659 - main_output_loss: 0.3512 - aux_output_loss: 0.4979 - val_loss: 0.4021 - val_main_output_loss: 0.3889 - val_aux_output_loss: 0.5216\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B],[y_train, y_train], epochs=20, validation_data=([X_valid_A, X_valid_B],[y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 29us/sample - loss: 0.3489 - main_output_loss: 0.3348 - aux_output_loss: 0.4693\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above for wide and deep is very static, however if you want to make it such that you are able to dive deeper and actually build a class around it, this is possible by using the Model class from keras. This is subclassing which all you need is the call function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation=keras.activations.relu, **kwargs):\n",
    "        super().__init__(**kwargs) # handles the standard arguments\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_ouput = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_ouput(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 3s 235us/sample - loss: 1.1076 - output_1_loss: 0.9924 - output_2_loss: 2.1426 - val_loss: 0.6589 - val_output_1_loss: 0.5956 - val_output_2_loss: 1.2292\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.5924 - output_1_loss: 0.5397 - output_2_loss: 1.0662 - val_loss: 0.5668 - val_output_1_loss: 0.5236 - val_output_2_loss: 0.9565\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.5067 - output_1_loss: 0.4663 - output_2_loss: 0.8705 - val_loss: 0.5293 - val_output_1_loss: 0.4958 - val_output_2_loss: 0.8316\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4761 - output_1_loss: 0.4431 - output_2_loss: 0.7726 - val_loss: 0.5326 - val_output_1_loss: 0.5057 - val_output_2_loss: 0.7756\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4565 - output_1_loss: 0.4282 - output_2_loss: 0.7105 - val_loss: 0.4942 - val_output_1_loss: 0.4678 - val_output_2_loss: 0.7324\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4498 - output_1_loss: 0.4247 - output_2_loss: 0.6756 - val_loss: 0.4757 - val_output_1_loss: 0.4517 - val_output_2_loss: 0.6926\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4335 - output_1_loss: 0.4099 - output_2_loss: 0.6453 - val_loss: 0.4630 - val_output_1_loss: 0.4416 - val_output_2_loss: 0.6568\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4192 - output_1_loss: 0.3963 - output_2_loss: 0.6258 - val_loss: 0.4566 - val_output_1_loss: 0.4369 - val_output_2_loss: 0.6357\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4166 - output_1_loss: 0.3959 - output_2_loss: 0.6048 - val_loss: 0.4614 - val_output_1_loss: 0.4416 - val_output_2_loss: 0.6403\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4025 - output_1_loss: 0.3815 - output_2_loss: 0.5920 - val_loss: 0.4414 - val_output_1_loss: 0.4223 - val_output_2_loss: 0.6137\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3996 - output_1_loss: 0.3797 - output_2_loss: 0.5787 - val_loss: 0.4349 - val_output_1_loss: 0.4168 - val_output_2_loss: 0.5980\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3977 - output_1_loss: 0.3787 - output_2_loss: 0.5678 - val_loss: 0.4372 - val_output_1_loss: 0.4190 - val_output_2_loss: 0.6014\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3894 - output_1_loss: 0.3708 - output_2_loss: 0.5579 - val_loss: 0.4364 - val_output_1_loss: 0.4188 - val_output_2_loss: 0.5958\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3872 - output_1_loss: 0.3690 - output_2_loss: 0.5505 - val_loss: 0.4313 - val_output_1_loss: 0.4157 - val_output_2_loss: 0.5728\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3799 - output_1_loss: 0.3622 - output_2_loss: 0.5393 - val_loss: 0.4231 - val_output_1_loss: 0.4075 - val_output_2_loss: 0.5642\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3781 - output_1_loss: 0.3608 - output_2_loss: 0.5328 - val_loss: 0.4208 - val_output_1_loss: 0.4040 - val_output_2_loss: 0.5722\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3770 - output_1_loss: 0.3603 - output_2_loss: 0.5267 - val_loss: 0.4136 - val_output_1_loss: 0.3983 - val_output_2_loss: 0.5524\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3707 - output_1_loss: 0.3541 - output_2_loss: 0.5196 - val_loss: 0.4141 - val_output_1_loss: 0.3988 - val_output_2_loss: 0.5524\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3679 - output_1_loss: 0.3520 - output_2_loss: 0.5128 - val_loss: 0.4070 - val_output_1_loss: 0.3927 - val_output_2_loss: 0.5364\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3892 - output_1_loss: 0.3748 - output_2_loss: 0.5202 - val_loss: 0.4088 - val_output_1_loss: 0.3947 - val_output_2_loss: 0.5362\n"
     ]
    }
   ],
   "source": [
    "model = WideAndDeepModel()\n",
    "model.compile(loss=[keras.metrics.mse, keras.metrics.mse], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD())\n",
    "history = model.fit([X_train_A, X_train_B],[y_train, y_train], epochs=20, validation_data=([X_valid_A, X_valid_B],[y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving a model, this will only work using the sequential and not the subclass done above.\n",
    "# model.save('my_model.h5')\n",
    "# model = keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using callbacks to save model at checkpoints if the model runs for a long time.\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # Have to do this first for the scaler to understand the data\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=keras.activations.relu, input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1) #output layer\n",
    "])\n",
    "\n",
    "model.compile(loss=keras.metrics.mean_squared_error, optimizer=keras.optimizers.SGD())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 1.0255 - val_loss: 37.2054\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 3.0399 - val_loss: 0.8442\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5086 - val_loss: 0.4090\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4119 - val_loss: 0.3811\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4230 - val_loss: 0.3728\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3845 - val_loss: 0.3675\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3940 - val_loss: 0.3599\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3765 - val_loss: 0.3816\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3980 - val_loss: 0.3465\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3598 - val_loss: 0.3437\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3570 - val_loss: 0.3437\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3568 - val_loss: 0.3377\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3975 - val_loss: 0.3928\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3794 - val_loss: 0.3488\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3668 - val_loss: 0.3429\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3645 - val_loss: 0.3439\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3580 - val_loss: 0.3420\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3575 - val_loss: 0.3388\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4046 - val_loss: 0.3461\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3735 - val_loss: 0.3404\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('my_keras_model.h5', save_best_only=True) #Saves the best model\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3686 - val_loss: 0.3424\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3562 - val_loss: 0.3352\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3526 - val_loss: 0.3335\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3510 - val_loss: 0.3416\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3514 - val_loss: 0.3351\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3488 - val_loss: 0.3326\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3474 - val_loss: 0.3331\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3468 - val_loss: 0.3289\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3477 - val_loss: 0.3314\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3460 - val_loss: 0.3286\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3435 - val_loss: 0.3295\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3435 - val_loss: 0.3263\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3498 - val_loss: 0.3291\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3417 - val_loss: 0.3271\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3402 - val_loss: 0.3255\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3393 - val_loss: 0.3257\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3417 - val_loss: 0.3249\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3378 - val_loss: 0.3282\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3434 - val_loss: 0.3315\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3477 - val_loss: 0.3269\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# If there is no change in the model accuracy after 10 epochs it stops the training\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can also write your own function for this.\n",
    "\n",
    "# class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "#     def on_epoch_end(self,epoch,logs):\n",
    "#         print (\"\\n val/train: {:.2f}\".format(logs[\"val_loss\"]/logs[\"loss\"])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tensorboard you can do some visualisations which is part of tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/30\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3363 - val_loss: 0.3230\n",
      "Epoch 2/30\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.3431 - val_loss: 0.3400\n",
      "Epoch 3/30\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3402 - val_loss: 0.3546\n",
      "Epoch 4/30\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3335 - val_loss: 0.3259\n",
      "Epoch 5/30\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3313 - val_loss: 0.3284\n",
      "Epoch 6/30\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3307 - val_loss: 0.3282\n",
      "Epoch 7/30\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3299 - val_loss: 0.3245\n",
      "Epoch 8/30\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3292 - val_loss: 0.3232\n",
      "Epoch 9/30\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3278 - val_loss: 0.3210\n",
      "Epoch 10/30\n",
      "11610/11610 [==============================] - 1s 59us/sample - loss: 0.3326 - val_loss: 0.3205\n",
      "Epoch 11/30\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3290 - val_loss: 0.3247\n",
      "Epoch 12/30\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3273 - val_loss: 0.3237\n",
      "Epoch 13/30\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3266 - val_loss: 0.3169\n",
      "Epoch 14/30\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3271 - val_loss: 0.3221\n",
      "Epoch 15/30\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3263 - val_loss: 0.3268\n",
      "Epoch 16/30\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3259 - val_loss: 0.3470\n",
      "Epoch 17/30\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3308 - val_loss: 0.3174\n",
      "Epoch 18/30\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3248 - val_loss: 0.3166\n",
      "Epoch 19/30\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3315 - val_loss: 0.3196\n",
      "Epoch 20/30\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3252 - val_loss: 0.3196\n",
      "Epoch 21/30\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3258 - val_loss: 0.3260\n",
      "Epoch 22/30\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3295 - val_loss: 0.3172\n",
      "Epoch 23/30\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3251 - val_loss: 0.3266\n",
      "Epoch 24/30\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3713 - val_loss: 0.3196\n",
      "Epoch 25/30\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3254 - val_loss: 0.3195\n",
      "Epoch 26/30\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3242 - val_loss: 0.3229\n",
      "Epoch 27/30\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3251 - val_loss: 0.3203\n",
      "Epoch 28/30\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3236 - val_loss: 0.3169\n",
      "Epoch 29/30\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3260 - val_loss: 0.3172\n",
      "Epoch 30/30\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3216 - val_loss: 0.3187\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid), callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d261b2338d7ee126\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d261b2338d7ee126\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we find the best hyperparameters? We can use the grid search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=keras.activations.relu))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=keras.metrics.mse, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 1s 83us/sample - loss: 1.1556 - val_loss: 0.3445\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3423 - val_loss: 0.3206\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3281 - val_loss: 0.3176\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3239 - val_loss: 0.3138\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3211 - val_loss: 0.3127\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3200 - val_loss: 0.3142\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3188 - val_loss: 0.3134\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3173 - val_loss: 0.3118\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3170 - val_loss: 0.3101\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3168 - val_loss: 0.3110\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3163 - val_loss: 0.3107\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3155 - val_loss: 0.3097\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3151 - val_loss: 0.3106\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3151 - val_loss: 0.3114\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3145 - val_loss: 0.3093\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3141 - val_loss: 0.3101\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3142 - val_loss: 0.3082\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3135 - val_loss: 0.3082\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3143 - val_loss: 0.3085\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3134 - val_loss: 0.3080\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3128 - val_loss: 0.3120\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3127 - val_loss: 0.3070\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3130 - val_loss: 0.3105\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3129 - val_loss: 0.3075\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3126 - val_loss: 0.3091\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3125 - val_loss: 0.3071\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3121 - val_loss: 0.3093\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3131 - val_loss: 0.3077\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3120 - val_loss: 0.3074\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3117 - val_loss: 0.3078\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3120 - val_loss: 0.3076\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3113 - val_loss: 0.3099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16034fc50>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 37us/sample - loss: 0.3045\n"
     ]
    }
   ],
   "source": [
    "mse_test = keras_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1691058 0.8301809 1.9018357]\n"
     ]
    }
   ],
   "source": [
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem however with this is that we do not want a grid search to be able to do one at a time, rather we would like a randomised search which will be better, the below will take hours, and so it will not be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import reciprocal\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# param_distribs = {\n",
    "#     \"n_hidden\": [0, 1, 2, 3],\n",
    "#     \"n_neurons\": np.arange(1, 100),\n",
    "#     \"learning_rate\": reciprocal(3e-4, 3e-2)\n",
    "# }\n",
    "\n",
    "# rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "# rnd_search_cv.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump_session('ch10ann.db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
